<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
  <style>
    body {
      padding: 100px;
      width: 1000px;
      margin: auto;
      text-align: left;
      font-weight: 300;
      font-family: 'Open Sans', sans-serif;
      color: #121212;
    }

    h1,
    h2,
    h3,
    h4 {
      font-family: 'Source Sans Pro', sans-serif;
    }
  </style>
  <title>CS 184 Final Project</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
</head>

<body>
  <br />
  <h1 align="middle">Final Project: Multi-shot High Dynamic Range Imaging</h1>
  <h2 align="middle">Phoebe Li, Harrison Tin, Tyler Zhu, Christopher Zou</h2>

  <div class="padded">
    <p>
      In our project, our goal is to be able to represent images with a high dynamic range of intensity values
      from a bunch of low dynamic range images. This is a classic problem that is widely utilized by modern phones,
      and our approach should be able to replicate most of the core parts of it in an end-to-end pipeline. We
      hopefully will even be able to upload our own burst photos and get out HDR images.
    </p>

    <h2 align="middle">Problem Description</h2>
    <p>
      Whether due to small camera aperture (such as on mobile phones), uneven lighting (such as in extremely bright or
      dark settings), or other factors, we often end up taking pictures in less-than-ideal light conditions. If
      evaluated using a single shot, these factors lead to overexposure or underexposure, both of which lead to
      information loss, noise, and decreased picture quality.
    </p>

    <p>
      Taking good pictures under all lighting conditions has strong commercial and aesthetic motivations. Common
      solutions to compensate for poor lighting include adding gain and introducing longer exposure times;
      unfortunately, gain amplifies noise and longer exposure times can introduce blurring [1].
    </p>

    <p>
      We want to try another solution: high dynamic range (HDR) photography, which involves quickly capturing, aligning,
      and merging several frames. At a high level, we will implement Debevec’s algorithm to merge multiple photographs
      taken at various exposures into one, higher dynamic range radiance map. Then, we will convert that map into
      viewable images and measure performance. Finally, we will then look to implement extensions on our original
      project that will reduce noise and improve efficiency.
    </p>

    <h2 align="middle">Goals and Deliverables</h2>
    <p>
      We plan to deliver an interactive Jupyter Notebook that walks the user through the different steps of generating
      HDR images. Our final deliverable can be broken down into the following tasks:
    </p>
    <ol>
      <li>
        Setup Jupyter Notebook codebase for image processing.
      </li>
      <li>
        Recover radiance map from a collection of images. We will use Debevec’s method in [2] to “fuse the multiple
        photographs into a single, high dynamic range radiance map whose pixel values are proportional to the true
        radiance values in the scene”.
      </li>
      <li>
        Convert radiance map into image viewable on a LDR display. We will perform global and local tone mapping
        following [3].
      </li>
      <li>
        Measure performance using reference images found <a
          href="http://cs.brown.edu/courses/csci1950-g/results/final/dkendall/images.html">here</a> and display results.
      </li>
      <li>
        Create an end-to-end pipeline for applying our method to pictures we take on our own phones.
      </li>
    </ol>

    <p>
      Once we finish our core codebase, we hope to deliver a number of extensions on top depending on which ones we
      think will be most valuable for improving how realistic our pictures look. Some ideas here that we are thinking of
      are the following:
    </p>
    <ul>
      <li>
        Auto-alignment to fix mis-aligned images which often happens in real world data (See the “Aligning Frames”
        section of [1]).
      </li>
      <li>
        A fast bilateral filtering optimization to speed up our process from the naive bilateral filtering we’ll
        implement above. Here is an example reference that we’d follow for <a
          href="http://cybertron.cg.tu-berlin.de/eitz/bilateral_filtering/index.html">bilateral filtering</a>.
      </li>
      <li>
        Different tone-mappings that will make our method more applicable to different types of images with greater
        color constancy/corrections.
      </li>
      <li>
        Exposure fusion, a simple way to create a cross-exposure image without needing HDR at all. We will be following
        [4].
      </li>
    </ul>
    <p>
      We will pick a few to work on that seem most promising. If time permits, we will also try to implement more parts
      of [1] that help our final result, or even re-implement that end-to-end pipeline.
    </p>

    <h2 align="middle">Schedule</h2>
    <p>
      Our proposed schedule is the following.
    </p>

    <p>
    <ul>
      <li>
        Week 1, 4/11 - 4/17
        <ul>
          <li>
            Setup codebase
          </li>
        </ul>
      </li>
      <li>
        Week 2, 4/18 - 4/24
        <ul>
          <li>
            Setup codebase
          </li>
          <li>
            Implement Debevec’s algorithm for recovering a radiance map
          </li>
          <li>
            Implement radiance map into viewable image
          </li>
          <li>
            Create pipeline for applying method on our own images
          </li>
        </ul>
      </li>
      <li>
        Week 3, 4/25 - 5/1
        <ul>
          <li>
            Implement auto-alignment
          </li>
          <li>
            Implement fast bilateral optimization
          </li>
          <li>
            Implement alternate tone mapping method(s)
          </li>
        </ul>
      </li>
      <li>
        Week 4, 5/2 - 5/8
        <ul>
          <li>
            Implement [1], if feasible
          </li>
          <li>
            Implement exposure fusion
          </li>
          <li>
            Polish the final, interactive deliverable
          </li>
        </ul>
      </li>
    </ul>
    </p>

    <h2 align="middle">Resources</h2>
    <p>
      We will be primarily using Python as our programming language, with the numpy and opencv packages for help. The
      main papers we will be following are [2] and [3]. We will also reference and potentially implement parts of
      [1], which was linked in the original specification.
    </p>

    <p>
      [1] “<a href="https://static.googleusercontent.com/media/hdrplusdata.org/en//hdrplus.pdf">Burst photography for
        high dynamic range and low-light imaging on mobile cameras.</a>” Samuel W. Hasinoff, Dillon
      Sharlet, Ryan Geiss, Andrew Adams, Jonathan T. Barron, Florian Kainz, Jiawen Chen, and Marc Levoy.
    </p>
    <p>
      [2] “<a href="http://www.pauldebevec.com/Research/HDR/debevec-siggraph97.pdf">Recovering High Dynamic Range
        Radiance Maps from Photographs.</a>” Paul E. Debevec and Jitendra Malik.
    </p>
    <p>
      [3] “<a href="http://people.csail.mit.edu/fredo/PUBLI/Siggraph2002/DurandBilateral.pdf">Fast Bilateral Filtering
        for the Display of High-Dynamic-Range Images.</a>” Frédo Durand and Julie Dorsey.
    </p>
    <p>
      [4] “<a href="https://mericam.github.io/papers/exposure_fusion_reduced.pdf">Exposure Fusion.</a>” Tom Mertens, Jan
      Kautz, and Frank Van Reeth.
    </p>
    <p>
      Dataset: ​​https://hdrplusdata.org/
    </p>
</body>

</html>